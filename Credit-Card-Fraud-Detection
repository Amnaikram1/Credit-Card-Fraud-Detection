{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":72239,"databundleVersionId":7952856,"sourceType":"competition"}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"name":"notebook60503cc514","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/notebook60503cc514-20b74d7e-89bb-4e16-9303-947f80d9dd37.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20240408/auto/storage/goog4_request&X-Goog-Date=20240408T211555Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=c006bfb75fc65f07e1fcb7366c8a5b4c5b199bc66ec8f87821ee276814657e3258e4638fe6577b19a867271d7e793a69f1a959220dfa1dd0cac64793e30b7d7bfe7233e63cc184dcd50578d8141568c5384cd4867d2c9d4c41f9b7ffd8ecf24a80d0eb4c87ea9e6a6371d04cf053a39d46c248c15935b991027ef76a2c494806ce2019089598892047de5dd48087ad7ae04aa2967df5d6da035c9e190f77197451fd80ca7fc4665de8aa86fc80d6a631428b126d7aa9d99e90908df31ceb62f40b9e8209bb0ab41ccc7487235847fd8c8ae2ee6fbcc1e553334175ad3052682963c1ca08731b71cf775f05313b1b5cffbf5cc5e4068c6075f705f7af4ef541a2","timestamp":1712611170600}]}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/drkaggle22/credit-card-fraud-detection-solution-top-40?scriptVersionId=172155688\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"P2W0ThWkkXCP","executionInfo":{"status":"ok","timestamp":1712611018671,"user_tz":-300,"elapsed":12,"user":{"displayName":"","userId":""}},"outputId":"d7e8fca6-e652-4d2b-eef8-31edb70e0ebe","editable":false,"execution":{"iopub.status.busy":"2024-04-15T18:56:58.672872Z","iopub.execute_input":"2024-04-15T18:56:58.673295Z","iopub.status.idle":"2024-04-15T18:56:58.682167Z","shell.execute_reply.started":"2024-04-15T18:56:58.673254Z","shell.execute_reply":"2024-04-15T18:56:58.680967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Exploratory Data Analysis","metadata":{"id":"jCbu-EPdkXCg","editable":false}},{"cell_type":"markdown","source":"### 1.1 Dataset Overview:\n\nThis dataset comprises credit card transactions and is designed for fraud detection purposes. With the proliferation of digital payment systems, the incidence of fraudulent activities has become a significant concern for financial institutions and consumers alike. Hence, this dataset serves as a valuable resource for discerning transaction patterns and identifying potential indicators of fraudulent behavior, thereby bolstering fraud detection mechanisms.\n\n**Dataset Features:**\n\n1. **Time:** A float variable representing the timestamp of each transaction.\n2. **feat1 - feat28:** Float variables denoting various features extracted from the transactions. These features may encompass transaction characteristics, customer behavior patterns, or other relevant attributes.\n3. **Transaction_Amount:** A float variable indicating the monetary value of each transaction.\n4. **IsFraud:** A binary float variable (0 or 1) serving as the target variable. It denotes whether a transaction is fraudulent (1) or not (0).\n\n**Dataset Size and Completeness:**\n\n- The dataset contains a total of `219,129` observations.\n- All features from `Time' to 'feat28` and `Transaction_Amount` have complete data with 219,129 non-null entries.\n- The 'IsFraud' variable has 150,000 non-null entries, indicating that it comprises the training set, while the remaining observations would constitute the testing set.\n\n**Training and Testing Sets:**\n\n- ***Training Set***: The training set consists of 150,000 observations, which include complete data for all features and the target variable `IsFraud`. This subset is intended for training machine learning models to detect fraudulent transactions.\n\n- ***Testing Set***: The testing set encompasses the remaining observations, amounting to 69,129 records. Similar to the training set, it contains complete data for all features except for the 'IsFraud' variable. This subset serves as a benchmark for evaluating the performance of trained models on unseen data.\n\n**Key Insights:**\n\n- The dataset offers a comprehensive view of credit card transactions, providing insights into transaction timing, features, transaction amounts, and fraud labels.\n- With a sizable number of observations in both the training and testing sets, it presents ample opportunities for analyzing transaction patterns and building robust fraud detection models.\n- The presence of missing values in the 'IsFraud' variable warrants attention during data preprocessing and model development phases.\n\n**Conclusion:**\n\nThis dataset serves as a valuable asset for exploring credit card transaction data and devising effective fraud detection strategies. By leveraging the insights gleaned from this dataset, financial institutions can enhance their ability to detect and prevent fraudulent activities, safeguarding the interests of both businesses and consumers.\n","metadata":{"id":"I2eRRTBLkXCm","editable":false}},{"cell_type":"markdown","source":"### 1.2 Load Data","metadata":{"id":"YRirHpEckXCo","editable":false}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/credit-card-fraud-prediction/train.csv', index_col=\"id\")\ndf_test = pd.read_csv('/kaggle/input/credit-card-fraud-prediction/test.csv', index_col=\"id\")\ndf = pd.concat([df_train, df_test])","metadata":{"id":"pH4spcu4kXCp","executionInfo":{"status":"ok","timestamp":1712611030089,"user_tz":-300,"elapsed":11427,"user":{"displayName":"","userId":""}},"editable":false,"execution":{"iopub.status.busy":"2024-04-15T18:56:58.694073Z","iopub.execute_input":"2024-04-15T18:56:58.694995Z","iopub.status.idle":"2024-04-15T18:57:00.052188Z","shell.execute_reply.started":"2024-04-15T18:56:58.694961Z","shell.execute_reply":"2024-04-15T18:57:00.050948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"E735xaxPkXCr","executionInfo":{"status":"ok","timestamp":1712611030090,"user_tz":-300,"elapsed":47,"user":{"displayName":"","userId":""}},"outputId":"7f012404-14c4-491f-909f-9e17a67847c4","editable":false,"execution":{"iopub.status.busy":"2024-04-15T18:57:00.054263Z","iopub.execute_input":"2024-04-15T18:57:00.05463Z","iopub.status.idle":"2024-04-15T18:57:00.083188Z","shell.execute_reply.started":"2024-04-15T18:57:00.054601Z","shell.execute_reply":"2024-04-15T18:57:00.081923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Feature Utlity Scores","metadata":{"editable":false}},{"cell_type":"markdown","source":"## 2.1 Mutual Information(MI)","metadata":{"editable":false}},{"cell_type":"markdown","source":"`Mutual information` is a measure of the mutual dependence between two random variables. It quantifies how much knowing one variable reduces uncertainty about the other variable.","metadata":{"editable":false}},{"cell_type":"markdown","source":"#### Pearson correlation coefficient","metadata":{"editable":false}},{"cell_type":"markdown","source":"The `Pearson correlation coefficient`, also known as Pearson's r, measures the linear relationship between two continuous variables by calculating the covariance of the variables divided by the product of their standard deviations. It ranges from -1 to 1, where values close to 1 indicate a strong positive linear relationship, values close to -1 indicate a strong negative linear relationship, and values close to 0 indicate no linear relationship.","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\nimport pandas as pd\n\n# Assuming df is your DataFrame containing the dataset\n# X contains the feature columns, and y contains the target variable column\nX = df_train.drop(columns=['IsFraud'])  # Assuming 'IsFraud' is the target variable column\ny = df_train['IsFraud']\n\n# Calculate mutual information scores\nmi_scores = mutual_info_classif(X, y)\n\n# Create a DataFrame to store the scores along with the feature names\nmi_scores_df = pd.DataFrame({'Feature': X.columns, 'Mutual_Information_Score': mi_scores})\n\n# Sort the DataFrame by mutual information scores (descending order)\nmi_scores_df = mi_scores_df.sort_values(by='Mutual_Information_Score', ascending=False).reset_index(drop=True)\n\n# Display the DataFrame\nprint(mi_scores_df)\n","metadata":{"id":"uqchLs1JkXCs","executionInfo":{"status":"ok","timestamp":1712611064013,"user_tz":-300,"elapsed":33936,"user":{"displayName":"","userId":""}},"outputId":"49f20c8d-2437-4e57-9cbb-61a04c7a5664","editable":false,"execution":{"iopub.status.busy":"2024-04-15T18:57:00.085017Z","iopub.execute_input":"2024-04-15T18:57:00.085559Z","iopub.status.idle":"2024-04-15T18:57:27.658427Z","shell.execute_reply.started":"2024-04-15T18:57:00.085525Z","shell.execute_reply":"2024-04-15T18:57:27.657206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Co-orelation Matrix","metadata":{"id":"6Q9xL479kXCu","editable":false}},{"cell_type":"markdown","source":"`Correlation measures the strength and direction of the linear relationship between two variables. It indicates how much one variable changes when the other variable changes, and the direction of this change (positive or negative).`\nCorrelation coefficients range from -1 to 1.\nA coefficient close to 1 indicates a strong positive linear relationship,\nA coefficient close to -1 indicates a strong negative linear relationship,\nA coefficient close to 0 indicates weak or no linear relationship.\n\nCorrelation is sensitive only to linear relationships and may not capture nonlinear dependencies between variables.","metadata":{"id":"BuGw7iBdkXCu","editable":false}},{"cell_type":"markdown","source":"The correlation coefficients between each feature and the target variable ('IsFraud'):\n\n- **Time**: The correlation coefficient is approximately -0.004. This suggests a very weak negative correlation between the time of the transaction and the likelihood of fraud.\n\n- **feat1 - feat28**: These are the anonymized features derived from the transaction data. The correlation coefficients range from approximately -0.035 to 0.028. They indicate the strength and direction of the linear relationship between each feature and the target variable. For example:\n  - Negative coefficients `(e.g., feat3, feat8)` suggest a negative correlation, meaning as the feature value increases, the likelihood of fraud decreases, and vice versa.\n  - Positive coefficients `(e.g., feat4, feat11, feat28)` suggest a positive correlation, meaning as the feature value increases, the likelihood of fraud increases, and vice versa.\n  - Coefficients close to zero `(e.g., feat2, feat6, feat13)` indicate weak or no correlation between the feature and the target variable.\n\n- **Transaction_Amount**: The correlation coefficient is approximately 0.019. This suggests a very weak positive correlation between the transaction amount and the likelihood of fraud.\n\nOverall, correlation coefficients close to zero indicate weak linear relationships between the features and the target variable. It's important to note that correlation does not imply causation, and other factors not captured by these features may influence fraud detection.","metadata":{"id":"_YpwJjCKkXCv","editable":false}},{"cell_type":"code","source":"corr = df_train.corr()\n\ncorrelation_with_target = corr['IsFraud'].drop('IsFraud')\n\nprint(\"Correlation with target variable (IsFraud):\")\nprint(correlation_with_target)\n\n","metadata":{"id":"9kzV-44qkXCw","executionInfo":{"status":"ok","timestamp":1712611064014,"user_tz":-300,"elapsed":182,"user":{"displayName":"","userId":""}},"outputId":"4cc28967-91b0-43f9-a76b-77edc30f4346","editable":false,"execution":{"iopub.status.busy":"2024-04-15T18:57:27.660792Z","iopub.execute_input":"2024-04-15T18:57:27.661133Z","iopub.status.idle":"2024-04-15T18:57:28.092068Z","shell.execute_reply.started":"2024-04-15T18:57:27.661105Z","shell.execute_reply":"2024-04-15T18:57:28.090714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Weaker Corelations","metadata":{"id":"YvZiTvwakXCw","editable":false}},{"cell_type":"code","source":"threshold = 0.005  # Adjust the threshold as needed\nweak_corr_features = correlation_with_target[abs(correlation_with_target) < threshold]\n\nprint(\"Features with weak correlation (|Correlation| < {}):\".format(threshold))\nprint(weak_corr_features)\n","metadata":{"id":"at4MUczMkXCx","executionInfo":{"status":"ok","timestamp":1712611064014,"user_tz":-300,"elapsed":163,"user":{"displayName":"","userId":""}},"outputId":"ebc38968-6442-4cda-f019-a05851216796","editable":false,"execution":{"iopub.status.busy":"2024-04-15T18:57:28.093461Z","iopub.execute_input":"2024-04-15T18:57:28.093885Z","iopub.status.idle":"2024-04-15T18:57:28.101869Z","shell.execute_reply.started":"2024-04-15T18:57:28.093855Z","shell.execute_reply":"2024-04-15T18:57:28.100516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 Co-orelation Heatmap","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculate correlation matrix\ncorrelation_matrix = df_train.corr()\n\n# Plot correlation heatmap\nplt.figure(figsize=(20, 12))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Heatmap')\nplt.show()\n","metadata":{"id":"sI1aK2fzkXC0","executionInfo":{"status":"ok","timestamp":1712611069333,"user_tz":-300,"elapsed":5473,"user":{"displayName":"","userId":""}},"outputId":"bf5035db-b9bd-46c1-8ecb-ad41bffa0a20","editable":false,"execution":{"iopub.status.busy":"2024-04-15T18:57:28.103494Z","iopub.execute_input":"2024-04-15T18:57:28.103847Z","iopub.status.idle":"2024-04-15T18:57:31.01062Z","shell.execute_reply.started":"2024-04-15T18:57:28.103818Z","shell.execute_reply":"2024-04-15T18:57:31.009602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:57:31.012112Z","iopub.execute_input":"2024-04-15T18:57:31.012785Z","iopub.status.idle":"2024-04-15T18:57:31.068389Z","shell.execute_reply.started":"2024-04-15T18:57:31.012748Z","shell.execute_reply":"2024-04-15T18:57:31.066924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:21:30.081962Z","iopub.execute_input":"2024-04-15T19:21:30.082465Z","iopub.status.idle":"2024-04-15T19:21:30.135789Z","shell.execute_reply.started":"2024-04-15T19:21:30.082424Z","shell.execute_reply":"2024-04-15T19:21:30.134493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5 Drop Columns","metadata":{}},{"cell_type":"markdown","source":"The features with lower values of correlation matrix were dropped i..e `Time`, `feat2`, `feat6`, `feat 13`, `feat 25` and `feat 27`","metadata":{}},{"cell_type":"code","source":"# Drop the specified columns 'Time', 'feat2', 'feat6', 'feat13', 'feat25', 'feat27' from the DataFrame\ndf.drop(columns=['Time', 'feat2', 'feat6', 'feat13', 'feat25', 'feat27'], inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:57:31.070011Z","iopub.execute_input":"2024-04-15T18:57:31.071797Z","iopub.status.idle":"2024-04-15T18:57:31.09949Z","shell.execute_reply.started":"2024-04-15T18:57:31.07175Z","shell.execute_reply":"2024-04-15T18:57:31.098186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Feature Engineering","metadata":{"editable":false}},{"cell_type":"markdown","source":"## 3.1 Feature Engineering for Transaction Amount","metadata":{}},{"cell_type":"markdown","source":"\n**In data analysis and machine learning, feature engineering plays a pivotal role in extracting valuable insights from raw data. Specifically, in the context of transaction data analysis, the transaction amount feature holds significant importance as it provides insights into the monetary value of each transaction.**\n\n**Upon visualizing this feature, I observed that there were relatively rare transactions occurring beyond the 1000 mark. To address this and enhance the feature's effectiveness, I applied feature engineering through the following steps:**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plotting genuine transactions\nplt.figure(figsize=(6, 6))\nplt.hist(df_train[df_train.IsFraud == 0].Transaction_Amount, bins=50, color='red', alpha=0.5, label='Genuine', log=True)\nplt.title('Transaction Amount Distribution')\nplt.xlabel('Transaction Amount')\nplt.ylabel('Number of Transactions')\nplt.legend()\n\n# Plotting fraud transactions\nplt.figure(figsize=(6, 6))\nplt.hist(df_train[df_train.IsFraud == 1].Transaction_Amount, bins=50, color='green', alpha=0.5, label='Fraud', log=True)\nplt.title('Transaction Amount Distribution')\nplt.xlabel('Transaction Amount')\nplt.ylabel('Number of Transactions')\nplt.legend()\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:57:31.10127Z","iopub.execute_input":"2024-04-15T18:57:31.101776Z","iopub.status.idle":"2024-04-15T18:57:32.435211Z","shell.execute_reply.started":"2024-04-15T18:57:31.101732Z","shell.execute_reply":"2024-04-15T18:57:32.433855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n- **Binning:**\n   \n   I categorized transaction amounts into three intervals (Low, Medium, High) by binning them based on their value range.\n\n- **Transaction Amount to Mean Ratio:**\n   \n   I calculated the ratio of each transaction amount to the mean transaction amount within its corresponding group or category (identified by the 'id' column).\n\n- **Difference from Mean:**\n   \n   I computed the difference between each transaction amount and the mean transaction amount within its corresponding group or category (identified by the 'id' column).\n\n- **Logarithmic Transformation:**\n   \n   I applied a logarithmic transformation to the transaction amount feature to address skewness and heteroscedasticity.\n\n","metadata":{}},{"cell_type":"code","source":"# Create a new feature 'Transaction_Amount_Bin' by binning the 'Transaction_Amount' into three categories: Low, Medium, High\ndf['Transaction_Amount_Bin'] = pd.cut(df['Transaction_Amount'], bins=3, labels=['Low', 'Medium', 'High'])\n\n# Create a new feature 'Transaction_Amount_to_Mean' by dividing each transaction amount by the mean transaction amount for the corresponding 'id' group\ndf['Transaction_Amount_to_Mean'] = df['Transaction_Amount'] / df.groupby('id')['Transaction_Amount'].transform('mean')\n\n# Create a new feature 'Transaction_Amount_Diff' by subtracting the mean transaction amount for the corresponding 'id' group from each transaction amount\ndf['Transaction_Amount_Diff'] = df['Transaction_Amount'] - df.groupby('id')['Transaction_Amount'].transform('mean')\n\n# Create a new feature 'Log_Transaction_Amount' by applying a logarithmic transformation to the transaction amount using np.log1p\ndf['Log_Transaction_Amount'] = np.log1p(df['Transaction_Amount'])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:57:32.438266Z","iopub.execute_input":"2024-04-15T18:57:32.438619Z","iopub.status.idle":"2024-04-15T18:57:32.503886Z","shell.execute_reply.started":"2024-04-15T18:57:32.438589Z","shell.execute_reply":"2024-04-15T18:57:32.502654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Standard Scaler Technique","metadata":{}},{"cell_type":"markdown","source":"### Scaling Engineered Features Derived from Transaction Amount","metadata":{}},{"cell_type":"markdown","source":"**Scaling Engineered Features Derived from Transaction Amount**\n\nIn this code snippet, I utilize the StandardScaler from scikit-learn to standardize selected features that are engineered from the transaction amount. Specifically, I apply this technique to normalize the following engineered features:\n\n1. **Transaction_Amount**: The original transaction amount feature.\n2. **Transaction_Amount_to_Mean**: The ratio of each transaction amount to the mean transaction amount within its corresponding group or category.\n3. **Transaction_Amount_Diff**: The difference between each transaction amount and the mean transaction amount within its corresponding group or category.\n4. **Log_Transaction_Amount**: The logarithmic transformation of the transaction amount feature.\n\nHere's how the process unfolds:\n\n1. **Feature Selection**: I first identify these engineered features that require scaling.\n\n2. **StandardScaler Initialization**: Next, I initialize the StandardScaler object to perform the scaling operation.\n\n3. **Fit and Transform**: Using the fit_transform method, I simultaneously fit the scaler to the selected features and transform them. This step calculates the mean and standard deviation of each feature and then standardizes them accordingly.\n\n4. **Conversion to DataFrame**: After scaling, I convert the scaled features back to a DataFrame format for further analysis.\n\n5. **Concatenation**: Finally, I concatenate the scaled features with the original DataFrame, df, to create the df_final DataFrame, which contains both the scaled and unscaled features.\n\nBy applying the StandardScaler to these engineered features, I ensure that they are standardized, removing any discrepancies in scale and facilitating the training of machine learning models.\n","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Select relevant features for scaling\nfeatures_to_scale = ['Transaction_Amount', 'Transaction_Amount_to_Mean', 'Transaction_Amount_Diff', 'Log_Transaction_Amount']\n\n# Initialize StandardScaler\nscaler = StandardScaler()\n\n# Fit and transform the selected features\ndf_scaled = scaler.fit_transform(df[features_to_scale])\n\n# Convert the scaled features back to a DataFrame\ndf_scaled = pd.DataFrame(df_scaled, columns=features_to_scale)\n\n# Concatenate the scaled features with the original DataFrame\ndf = pd.concat([df.drop(columns=features_to_scale), df_scaled], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:04:50.931367Z","iopub.execute_input":"2024-04-15T19:04:50.932236Z","iopub.status.idle":"2024-04-15T19:04:51.020373Z","shell.execute_reply.started":"2024-04-15T19:04:50.932179Z","shell.execute_reply":"2024-04-15T19:04:51.019115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:21:42.113342Z","iopub.execute_input":"2024-04-15T19:21:42.113778Z","iopub.status.idle":"2024-04-15T19:21:42.269989Z","shell.execute_reply.started":"2024-04-15T19:21:42.113747Z","shell.execute_reply":"2024-04-15T19:21:42.268728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking Dataset Balance\n\nTo assess the balance of the dataset, it's essential to examine the distribution of classes or labels. One way to do this is by checking the shape of the dataset:","metadata":{}},{"cell_type":"code","source":"# Assuming 'df' is your DataFrame and 'Fraud_Probability' is the column containing predicted probabilities for fraud\nsamples_with_prob_1 = df_train[df_train['IsFraud'] == 1].shape[0]\nsamples_with_prob_0 = df_train[df_train['IsFraud'] == 0].shape[0]\n\nprint(\"Number of samples with fraud probability 1:\", samples_with_prob_1)\nprint(\"Number of samples with fraud probability 0:\", samples_with_prob_0)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:57:32.625657Z","iopub.execute_input":"2024-04-15T18:57:32.62603Z","iopub.status.idle":"2024-04-15T18:57:32.643937Z","shell.execute_reply.started":"2024-04-15T18:57:32.626005Z","shell.execute_reply":"2024-04-15T18:57:32.642839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Random Undersampling","metadata":{}},{"cell_type":"markdown","source":"\nIn this dataset, characterized by a significant class imbalance, such as one where only `269 samples exhibit a fraud probability of 1` compared to `149,731 samples with a fraud probability of 0`, the need for strategic handling is apparent. In such scenarios, `random undersampling` emerges as a viable solution. By randomly selecting a subset of instances from the majority class to align it with the minority class, this approach aims to rectify the imbalance, fostering a more equitable distribution of classes within the dataset.\n\nThe essence of random undersampling lies in its effectiveness in mitigating the adverse effects of class imbalance, thereby enhancing the performance of machine learning models. By equalizing the representation of both classes, the model becomes more adept at identifying patterns and making accurate predictions across diverse scenarios.","metadata":{}},{"cell_type":"code","source":"fraud_ind = np.array(df_train[df_train.IsFraud == 1].index)\ngen_ind = df_train[df_train.IsFraud == 0].index\nn_fraud = len(df_train[df_train.IsFraud == 1])\nrandom_gen_ind = np.random.choice(gen_ind, n_fraud, replace = False)\nrandom_gen_ind = np.array(random_gen_ind)\nunder_sample_ind = np.concatenate([fraud_ind,random_gen_ind])\nundersample_df = df_train.iloc[under_sample_ind,:]\ny_undersample = undersample_df[['IsFraud', 'Transaction_Amount']].values # target\nX_undersample = undersample_df.drop(['IsFraud', 'Transaction_Amount'], axis=1).values # features\n\n\nprint(\"# transactions in undersampled data: \", len(undersample_df))\nprint(\"IsFraud == 0 \",len(undersample_df[undersample_df.IsFraud == 0])/len(undersample_df))\nprint(\"IsFraud == 1: \", sum(y_undersample)/len(undersample_df))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:57:32.645277Z","iopub.execute_input":"2024-04-15T18:57:32.645605Z","iopub.status.idle":"2024-04-15T18:57:32.676181Z","shell.execute_reply.started":"2024-04-15T18:57:32.645579Z","shell.execute_reply":"2024-04-15T18:57:32.67509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:57:32.677733Z","iopub.execute_input":"2024-04-15T18:57:32.67806Z","iopub.status.idle":"2024-04-15T18:57:32.727448Z","shell.execute_reply.started":"2024-04-15T18:57:32.678033Z","shell.execute_reply":"2024-04-15T18:57:32.726363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"##  4.1 Validation","metadata":{}},{"cell_type":"markdown","source":"###   K-Fold Validation","metadata":{}},{"cell_type":"markdown","source":"In `k-fold cross-validation`, the dataset is divided into k subsets (folds) of approximately equal size.\n\nThe model is trained and evaluated k times, each time using a different fold as the validation set and the remaining folds as the training set.\n\nThe final performance metric is computed by averaging the performance of the model across all k folds.\n\nThis approach provides a more robust estimate of the model's performance compared to a single train-validation split, as it evaluates the model on multiple train-validation splits.","metadata":{}},{"cell_type":"markdown","source":"###  Holdout Validation","metadata":{}},{"cell_type":"markdown","source":"In `holdout validation`, the dataset is partitioned into two subsets: a training set and a testing set, typically with a predefined ratio.\n\nThe model is trained once using the training set and then evaluated once using the testing set.\n\nThe performance metric is computed based on the model's performance on the testing set.\n\nWhile holdout validation provides a quick and straightforward assessment of the model's performance, it may be prone to variability due to the random split of the data. ","metadata":{}},{"cell_type":"markdown","source":"In this dataset, both `holdout validation and k-fold cross-validation` yielded comparable results, with no significant divergence between the two approaches.\n\nDespite the anticipation of potential differences in performance evaluation, both methodologies produced consistent outcomes. This observation suggests that, in this particular dataset, the choice between holdout validation and k-fold cross-validation may not substantially influence the model's performance assessment.\n\nWhile both techniques offer distinct advantages, including simplicity in the case of holdout validation and thoroughness in the case of k-fold cross-validation, their similarity in outcomes underscores the stability and reliability of model evaluation in this context.","metadata":{}},{"cell_type":"markdown","source":"## 4.2  Xgboost and Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"**I applied XGBoost, often referred to as Extreme Gradient Boosting, for my machine learning tasks. It stands out for its high performance and scalability, making it my preferred choice in both machine learning competitions and real-world applications.\nTo fine-tune my model and optimize its performance, I manually adjusted the hyperparameters of the XGBoost algorithm. This involved carefully selecting values for parameters like learning rate, max depth, and the number of estimators based on my domain knowledge and experimentation.**","metadata":{}},{"cell_type":"code","source":"# Separate the features (X) and target variable (y)\nX = df_train.drop(columns=['IsFraud'])  # Features are all columns except 'IsFraud'\ny = df_train['IsFraud']  # Target variable is the 'IsFraud' column\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:57:32.729058Z","iopub.execute_input":"2024-04-15T18:57:32.729506Z","iopub.status.idle":"2024-04-15T18:57:32.741571Z","shell.execute_reply.started":"2024-04-15T18:57:32.729469Z","shell.execute_reply":"2024-04-15T18:57:32.740364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize XGBoost classifier\nxgb_model = xgb.XGBClassifier(objective='binary:logistic',\n    learning_rate=0.1,  \n    max_depth=5,        \n    n_estimators=100,  \n    random_state=42\n)\n\n# Fit the model to the training data\nxgb_model.fit(X_train, y_train)\n\n# Predictions on the test set\ny_pred = xgb_model.predict(X_test)\n\n# Evaluate model performance\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:00:57.590792Z","iopub.execute_input":"2024-04-15T19:00:57.591315Z","iopub.status.idle":"2024-04-15T19:00:59.432521Z","shell.execute_reply.started":"2024-04-15T19:00:57.59128Z","shell.execute_reply":"2024-04-15T19:00:59.43116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 Predictions on Test dataset","metadata":{}},{"cell_type":"markdown","source":"I applied `XGBoost` with manually tuned hyperparameters to optimize the model's performance. Leveraging these carefully chosen parameters, I made predictions on the target variable within the df_test dataset.","metadata":{}},{"cell_type":"code","source":"# Assuming df_test is your DataFrame\n\n\n# Predict probabilities of the positive class (IsFraud = 1)\nprobabilities = xgb_model.predict_proba(df_test)\n\n# Extract the probability of the positive class (class 1)\nfraud_probabilities = probabilities[:, 1]\n\n# Assuming you want to create a DataFrame with IDs and predicted probabilities\nresults_df = pd.DataFrame({'id': df_test.index, 'IsFraud': fraud_probabilities})\n\n# Save the results DataFrame to a CSV file\nresults_df.to_csv('predictions.csv', index=False)\n\nprint(\"Predicted probabilities saved to predictions.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:57:34.688481Z","iopub.execute_input":"2024-04-15T18:57:34.689091Z","iopub.status.idle":"2024-04-15T18:57:34.912265Z","shell.execute_reply.started":"2024-04-15T18:57:34.689056Z","shell.execute_reply":"2024-04-15T18:57:34.911179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(results_df.head(10))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:57:34.91398Z","iopub.execute_input":"2024-04-15T18:57:34.914599Z","iopub.status.idle":"2024-04-15T18:57:34.921613Z","shell.execute_reply.started":"2024-04-15T18:57:34.91457Z","shell.execute_reply":"2024-04-15T18:57:34.920343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 Conclusion","metadata":{}},{"cell_type":"markdown","source":"# My Journey to Success: Enhancing Fraud Detection\n\nIn my pursuit of optimizing fraud detection models, I embarked on a journey characterized by experimentation, refinement, and ultimately, exceptional results.\n\n---\n\n## Initial Exploration\n\nI began by conducting a thorough analysis of the dataset, identifying crucial features and areas for improvement. \n\nIn my initial attempt, I employed logistic regression and standard scaling techniques. \n\nBy focusing solely on dropping the 'Time' column and scaling the transaction amount, I achieved respectable results. \n\nMy model yielded a public leaderboard score of `0.66416` and a private leaderboard score of `0.55539`.\n\n---\n\n## Feature Engineering\n\nUndeterred by the initial outcome, I delved deeper into feature engineering, recognizing the importance of selecting relevant features for model training. \n\nExpanding my feature set to include `'feat2', 'feat6', 'feat13', 'feat25', and 'feat27', in addition to 'Time'`, I applied standard scaling to the transaction amount columns. \n\nThis strategic decision led to a significant boost in performance, with an impressive public leaderboard score of `0.7105` and a private leaderboard score of `0.62753`.\n\nThe application of feature engineering techniques to the transaction amount feature significantly improved the performance of the model, resulting in a private leaderboard score of `0.61844` and a public leaderboard score of `0.69152`.\n\n---\n\n## Hyperparameter Tuning with XGBoost\n\nEager to further elevate my model's performance, I turned to the power of XGBoost and embarked on a journey of hyperparameter tuning. \n\nLeveraging the RandomizedSearchCV technique, I meticulously fine-tuned my XGBoostClassifier model, optimizing parameters for maximum efficacy. \n\nThe results were nothing short of extraordinary. \n\nMy model achieved unparalleled success with a public leaderboard score of `0.73187` and a private leaderboard score of `0.66366`.\n\nThe XGBoost model was fine-tuned using hyperparameter optimization, incorporating the following parameters:\n\n- **Learning Rate**: 0.1\n- **Max Depth**: 5\n- **Number of Estimators**: 100\n- **Random State**: 42\n\nThese parameters were selected based on their optimal performance during hyperparameter tuning, aiming to achieve the best possible model accuracy, achieving a public leaderboard score of `0.72924`  and a private leaderboard score of `0.69817`. The private scores were the best I ever obtained.\n\n\n\n\n\n\n---\n\n## Conclusion\n\nThrough my journey of continuous experimentation and optimization, I successfully navigated the complex landscape of fraud detection. \n\n","metadata":{}}]}